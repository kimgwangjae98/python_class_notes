수
정수 - 양수, 음수, 0
자연수 - 양수
유리수 - 유한히 표현되는 수
무리수 - 무한히 표현되는 수
허수 - i

디지털 - 0과 1로 된 이산적인 값(불연속적인 값). 아날로그 값을 쪼개어 만들기도 함. ADC(아날로그/디지털 컨버터)도 있음. 점으로 표현됨
아날로그 : 연속적인(무한한) 값. 선으로 표현됨. 아날로그 이산화를 샘플링 이라고 함. 
헤르츠(Hz) - 1ch ehddks 1wnrl rksms ghlttn
-> 배운 이유 : 사진 인식을 픽셀(디지털)로 하기 때문. 대체로 신경망 학습이 잘 되게 할려면 디지털 입력을 아날로그 입력처럼 만들어야 학습이 잘 되기 때문. 게다가 컴퓨터는 디지털로 연산을 하기에 배움

numpy - 선형대수 계산 라이브러리(행렬연산). 효율적으로 방정식을 풀 수 있도록 도와줌.
기호
그레디언트 - 역삼각형. 편미분들을 모아 만든 열벡터. 경사도벡터
라운드 - 6처럼 생긴 기호. 편미분
에타 - n처럼 생긴 기호
N - 자연수. 무한하고 셀수있음
Z - 정수. 자연수, 음수, 0 들을 의미. 무한하고 셀수있음
Q - 유리수. 분자분모꼴로 정수를 갖는 분수로 나타낼 수 있는 수. 무한하고 셀수있음
무리수는 기호 없음
R - 실수. 무리수+유리수. 무한하고 셀수없음(무리수때문). 

함수 - 입력과 출력의 관계를 의미함. 1개 입력에 여러개 출력 불가. 1개 입력에 1개 출력이면 일대일 대응.
domain - X. 입력. 정의역
codomain - Y. 출력. 공역
rectangular coordinate system - 직교 좌표계. (x,y,z). cartesian coordinate system
시그마 - 합
^n - 거듭제곱
다항 함수 - 여러개의 덧셈으로 연결되어 만들어진 식. c1x1 + c2x2 + c3x3 = f(x)
다차 함수 - 최고차항이 n차인 함수. x^n
지수 함수 - 거듭제곱 꼴의 함수.  a^x
로그함수 - logx
시그모이드 함수 - 1/(1 + e^-x)
다변수 함수 - 변수가 여러개인 함수
벡터 함수 : 입력이 벡터인 함수.
항 - 덧셈으로 연결되는 식
계수 - 변수에 붙은 숫자 cx
차수 - 변수의 제곱횟수 x^n
편향 - 다항 함수에서의 상수 +b
logarism - 값의 변화가 크다, 가파르다. 지수함수의 역함수

스칼라 - 크기(양)을 나타냄
벡터 - 크기+방향. 좌표계처럼 표시. 요소로 분해가능. 
단위 벡터 - u. 크기가 1인 벡터
전치 - 행과 열을 바꿈. 반시계 방향으로 돌림

스칼라 함수 - 그래프 차원 결정은 서로다른 변수의 갯수로 결정(x,y,z-3차원). 그래프 형태 결정도 서로다른 변수의 갯수로 결정
벡터 함수 -   그래프 차원 결정을 요소 갯수로 결정 (x=u,v,k- 3차원). 그래프 형태 결정은 서로다른 변수(벡터)의 갯수로 결정
합성 함수 - 여러개의 함수(레이어)가 합성된 함수. 신경망에 씀

변화율 - 미분을 나타냄
미분 - 작게 나눈다는 뜻. 0에 가까운 간격으로 나눔.
도함수 - 함수를 미분한 결과

학습 과정
1. 손실함수 선언
2. 손실함수 최소값 계산
3. 손실함수의 도함수 값이 0이 되도록 가중치 w 조정(w 조정을 w변화율 구해서 함)

if  변화량 < 0 : 손실함수가 작아지는 방향으로 이동
민감도 - 기울기 변화로 인해 값이 변함.  
라이프니츠 표기법 - f'(x)
뉴턴 표기법 - df(x)/dx
자코비안 행렬 - 입력과 결과의 숫자가 점점 달라지는 행렬. 입력은 열, 출력을 행으로 만큼
자동미분 툴 - 파이토치, 텐서플로

선형대수학 - 벡터 등을 연구하는 대수학의 한 분야. 벡터 변환에 관한 학문. 
인덱스 - 행렬의 한 요소
행 - 가로줄 갯수. m
열 - 세로줄 갯수. n
정방행렬 - 열갯수=행갯수

머신러닝 단계
1. 입력
    우리 일상에 쓰이는 모든 정보는 아날로그 데이터인데, 아날로그 데이터는 용량이 너무 크고 연속적이라 행렬과 벡터로 표현되는 디지털 기반인 컴퓨터에서는 쓸 수가 없음. 그렇기 때문에 데이터 전처리를 통해 아날로그 데이터를 디지털 데이터로 변환을 해줘야 머신러닝에 쓸 수 있음.
2. 함수
    전처리된 데이터(디지털 데이터)를 함수의 입력으로 삼아 함수를 결정함. 함수를 결정하는 방법은 함수의 결과를 손실함수의 입력으로 하여 계산된 오차값을 통해 손실함수의 미분값을 구하고, 이 미분값으로 함수의 가중치를 갱신하는 방법이다. 이 함수가 신경망을 쓰는지 안쓰는지에 따라 딥러닝 유무가 갈린다.
3. 예측
    완성된 함수(학습완료된 함수)에 입력을 넣어 정답과 비교해 예측을 실시한다. 만약 예측이 자신이 기대한 실제 성능보다 부족하다면 2. 함수 를 다른 방법으로 결정해야 할 것이다. 혹은 1. 입력의 데이터 전처리 방식을 바꾸던가.

수학을 왜 배워야 하나?
*알고리즘(컴퓨터가 정보를 처리하는 방식)을 설계할 때 수학이 사용되기 때문임.*
이 알고리즘을 손으로 설계할지, 신경망 함수를 사용하여 만들지, 아니면 다른거 쓸지는 자유임. 알고리즘은 머신러닝, 딥러닝은 알고리즘의 한 종류일 뿐. 우리가 머신러닝을 할 때 필요한 수학에는 함수, 미분, 스칼라와 벡터, 선형대수학(행렬)이 있음.

우리가 배울 수학의 흐름
1. 함수
    함수는 입력을 받으면 결과를 출력하는 수학 식임. 대표적으로 f(x) 로 표현함. 함수는 입력을 받으면 반드시 출력이 하나여야함. 입력에 사용되는 변수에 따라 함수를 구분함. 다항함수(덧셈으로 이어진 숫자의 갯수n에 따라 n항 함수라 부름), 다차함수(식에 쓰인 변수들 중에 가장 높은 차수n에 따라 n차 함수라 부름), 일변수함수(식에 변수가 한 종류만 있음), 다변수함수(식에 변수가 두 종류 이상이 있음) 가 있음. 또한 입력이 스칼라, 벡터인지에 따라 스칼라 함수, 벡터 함수가 있음.
2. 스칼라와 벡터
    스칼라는 크기를 표현하고 벡터는 크기와 방향을 둘 다 표현한다. 대체로 스칼라를 먼저 배우고 벡터를 배운다. 벡터는 함수 기호 위에 화살표를 추가해 표현하며 행렬로 표현된다. 벡터는 분해 가능하며 분해된 벡터를 요소라고 한다. 크기가 1인 벡터를 단위벡터라고 한다.
3. 미분
    미분은 변화율을 나타낸다. 두 간격 사이의 값이 얼만큼 변하느냐에 따라 값이 달라지며 두 점 사이의 간격이 크냐, 아니면 0에 가까울 정도로 작냐에 따라 평균변화율, 순간변화율이라고 한다. 대체로 잘게 쪼개어 계산한다. 컴퓨터는 0에 가까워지는 극한의 개념을 사용하기 어려워 중간 차등과 같이 근사를 통해 순간변화율을 계산한다. 미분값이 양수이면 값이 증가하고 있다는 뜻이고, 미분값이 음수이면 값이 감소하고 있다는 뜻이다.
4. 스칼라&벡터와 함수의 관계
    일변수 스칼라 함수 -> f(x) = y. 함수가 선으로 표현됨
    다변수 스칼라 함수 -> f(x,y,z) = k 같은거. 이 경우엔 함수가 덩어리로 표현됨.
    일변수 벡터 함수 -> 벡터 변수가 한 종류일 때를 말함. 
    다변수 벡터 함수 -> 벡터 변수가 둘 이상의 종류일 때를 말함.
    스칼라 함수는 변수 종류에 따라 그래프 차원과 함수의 모양이 결정됨
    벡터 함수는 벡터 변수에 사용된 요소 갯수에 따라 그래프 차원이 결정되고 벡터 변수 종류에 따라 함수의 모양이 결정됨.
    머신러닝의 함수는 다변수 벡터 함수가 사용됨
5. 미분과 함수의 관계
    일변수 - 도함수
    다변수 - 편도함수
    합성함수면 체인룰을 통해 미분을 실시
    각종 함수 공식이 있음. 
    머신러닝의 학습 중에 역전파 기법에 사용되어 가중치w를 구하는데 핵심적인 역할을 함.
6. 행렬(선형대수학)
    벡터는 전처리된 데이터 입력과 다변수 벡터 함수의 효율적인 계산을 위해 사용됨. 


numpy - 선형대수를 쉽게 풀어주는 계산 라이브러리.
얕은 복사 - 변수만 생성한 다음 원본 배열의 주소안에 있는 값들을 가져옴. reshape, asarray
깊은 복사 - 원본배열을 복사한 다음 변경시킴. array
브로드 케스팅 - 자동 크기 맞춤. 필터 복사.
내적 - 안으로 쌓는다. 두 벡터가 얼마나 평행한지를 나타냄
외적 - 밖으로 쌓는다. 두 벡터가 만드는 면이 얼마나 큰 정도를 나타냄. 두 벡터를 이용해 새로운 벡터를 만들어냄
1-1 내적 (dot product)
    정의 : 두 벡터가 크기와 방향의 관계를 고려하여 *스칼라값*을 반환하는 연산.
    두 벡터가 서로의 방향이 얼마나 "내부적"으로 비슷한지 정도를 나타냄. 즉슨 두 벡터가 서로 얼마나 평행한지 나타냄. 
    A.B = A와 B의 크기의 곱에 cos값을 취한 값. cos값은 A와 B의 각도임. 
1-2 외적 (cross product)
    정의 : 두 벡터의 방향을 고려하여 새로운 *벡터*를 만들어내는 연산
    두 벡터의 '외부'에 새로운 벡터가 생성됨. 즉슨 두 벡터가 이루는 평면에 수직인 벡터를 나타냄.

선형 결합 - c1x1+...cnxn 의 형태를 띰.
선형 독립 - 종속이 아닌 상태를 말함. 행렬에서 해가 하나
선형 종속 - 벡터들로 기존에 있던 벡터를 구할 수 있는 상태를 말함. 행렬에서 해가 없거나 무수히 많음
벡터 공간 - 선형 결합으로 만들어진 공간(여러 벡터로 만든 공간). 행렬로 표현됨.
부분 공간 - 벡터 공간의 일부. 경우의 수를 전부 합한 것

선형 결합(linear combine)
    정의 : 여러 벡터의 합으로 새로운 벡터를 표현, 만약 여러 벡터들 중에서 특정 벡터가 다른 벡터와 선형결합으로 표현된다면 그 벡터들의 선형 종속이다.
    => 독립과 기저를 정의하기 위함임, + 기호는 덧셈이 아니라 붙어있다는 뜻임.

랭크(rank)
    정의 : 여러 벡터들로 포함된 집합이 생성할 수 있는 공간의 차원의 갯수
    =>특징 : 랭크는 그 행렬에서 선형 독립인 행이나 열벡터의 갯수

랭크, 기저, 기저 벡터, 생성(span)
기저 - 벡터 갯수만큼 차원을 만들 수 있는(=선형독립) 벡터 집합.
기저 벡터 - 기저에 있는 벡터들 중에 하나
생성 - 주어진 벡터 집합으로부터 생성가능한 모든 경우. 주어진 벡터집합이 기저일때 생성을 하면 전체 기저벡터 갯수만큼 차원이 생성된다. 계수 c가 존재함.
행렬식 - 어떠한 스칼라값으로의 변환이고 이건 해의 관계를 알기위해 배웠다.
고유값과 고유벡터는 Av=ov 로 맵핑하는 차원 변환을 알기위해 배웠다.
선형방정식은 정사각 행렬, 직사각 행렬이 있다.