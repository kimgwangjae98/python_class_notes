"8월 26일"
AI(인공지능) → AGI(일반 인공지능) - 강인공지능, NARROW AI (좁은 인공지능) - 약인공지능

일반 AGI, NARROW는 TASKS로 구분함(용도). NARROW는 자율주행이나 CCTV 같은것임.

AGI가 가야할 길은 LLM(large laegue model)

왜 model인가? - 함수(funtion)이기 때문임. 입력→출력 이 나옴. 프레스기에 쓰는 틀도 모델이라고 볼수있음. 즉, 구조를 설계하는 것이 모델!

자연어(NLP) - natural language. 우리가 일상적으로 사용하는 언어. P는 프로세스 약자. 

언어모델이면 우리가 한말이 들어가면 우리가 알아먹을수 있는 말이 나옴.

CONVERSION(대화), TRANSRATE(번역), TTS가 도메인의 예임. 도메인이 엄청 많아서 선택과 집중이 필요함. 모델 선택이 그래서 중요하지.

왜 LARGE란 말이 왜 붙은거지. 왜 크다고 하지? 

—> 옛날에는 키울수록 서능 좋아지다 성장 안하는데 이젠(LARGE)는 키울수록 성능이 계속해서 좋아지기 때문이다. CHATGPT(GPT는 알고리즘)

왜 LARGE로 갈 수 있었나? 트랜스포머(2017) 가 나왔기 때문! 이전에는  CNN(1970년), 퍼셉트론(1940년) 이 나왔음. 이런 인공지능 발전은 퀀텀점프(한번에 성장). 왜 트랜스포머가 아직까지 쓰이는지는 나중에 배울거임.

왜 이렇게 텀이 길었나면 하드웨어의 한계 때문임. 하드웨어랑 인공지능이랑 같이 성장함.

AI → 머신러닝(신경망X), 딥러닝(신경망0).

AI 학습 분류 →지도학습(라벨이란 정답지가 있음supervised learning), 비지도학습(정답안줌), 강화학습(머신 딥러닝 다 씀). 이 분류는 라벨이란 정답지를 붙이냐 안붙이냐임.

개랑 고양이 분류는 지도학습, 비지도학습(일단 애기들도 개랑 고양이 몰라도 구분해냄), 강화학습(개만 보여줘도 고양이 구분 가능) 다 가능함.

비지도학습은 모델 성능이 안좋아서(특징 사람만큼 구분못함) 잘 안씀. 나중에 잘 쓸수도? 그래서 지도학습을 자주씀.

TASK따라 -강인공, 약인공

모델따라 - 머신, 딥러닝

머신과 딥러닝에는 학습법이 3가지있다.

도메인 - 인공지능은 사람을 본뜬거. 얘네 사람 모방함. 도메인은 AI에 적용할 분야를 말함(국어, 수학, 영어, 의학 등등…)

강인공지능

GPT 쓰는 법은 연습해야한다.

→ 사고력(생각)을 안하는 사람이기 때문. 사고의 깊이가 낮은 사람. 명확하게 말을 하지 않으면 GPT가 대답을 못함. 그래서 명확하게 말을 할 줄 알아야됨. 조건 추가같은거 말이지. 아님 현재상황을 말해주거나. 그 기법이 바로 프롬프트 엔지니어링. 1~2년 주니어 개발자보다 GPT가 코드 잘 짠데. 

로봇이나 자동차에 LLM적용해서 움직이게 하는 시도도 있음.

---

뇌와 척수가 사람에게 가장 중요한 명령체계?임

뇌에서 뉴런 네트워크 발상이 나옴. 뉴런은 전기적인 신호의 전달(tensor flow)을 함. 여기서 따온 것이 텐서플로우. 딥러닝 어플

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/914cd412-d6e9-4c46-8afb-88e9c6594671/3f5004b4-b409-4560-a56c-adce069bb999/image.png)

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/914cd412-d6e9-4c46-8afb-88e9c6594671/0ca354f4-b90a-4b09-abc0-2af1b82fa54c/image.png)

가지돌기에서 온 전기적 신호들을 모아서 축삭돌기로 전기신호를 전달함. 재분극, 탈분극 작용으로 전기적 신호가 팡! 터져서 전달됨. 일정한 역치(쓰레스홀드)를 넘어야 신호를 전달함. 이 역치를 뉴런의 베버상수가 결정함. 가지돌기→축삭돌기로 전기신호가 감. 역으로는 못감(단방향 통신. 다이오드같다). 역치를 넘냐 안넘냐가 중요함. 

재활운동(신경 끊어짐)을 하면 신경 잘린거말고 다른 신경으로 우회를 한다고함. 이게 의지력이란다 강사가… 메커니즘은 모른데. 근육신경이 많으면 근육이 힘 더쓸수있데(실전압축근육)

신경 3개 가 있고 그 합이 역치 넘어야 출력값이 넘어감. 그러니까, 신경 3개에서 1값을 오나서 역치 2(활성함수)를 넘으면 3-2로 1이 결과로 넘어감. 

wx….+wx ++a = y 에서 w는 가중치(뉴런에 세포가 얼마나 모여있는지 표현. 우리가 수정할 수 있는 값이기도 함), x는 신호(입력), a는 역치(이거넘어야 출력됨), y가 결과값 출력. y^이 예측값

딥러닝이란 신경망을 써서 만듬. 그걸로 우리가 보는 일차식같은걸 만들어 예측하는데 사용함. 만약 일차식이 안된다(단층 퍼셉트론이 안된다)? → 2차식을 만들어보자()

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/914cd412-d6e9-4c46-8afb-88e9c6594671/eecbee54-1d5e-4840-bf86-5bcd8f8680b4/image.png)

아벨방정식-4차 이상엔 해가없다! 즉, 내가 원하는 값을 원한다면 원하는 값에 가깝게(수렴시킴) 만들면 된다.

중요 퀴즈 : 왜 활성화 함수h(x)는 비선형 함수를 쓸까?

답 : 활설화 함수가 일차함수이면, 일차함수를 아무리 곱해서 똑같은 일차함수가 나옴. 그러니 이차 이상인 함수를 곱해야됨.

h(x)=y 인 활성화 함수는 2차이상(비선형, 미분) 지수함수도 가능함. 근데 왜 계단이냐? 역치넘었을 때 출력되는 값을 다음 계산에 쓰기위해. 

y-y^은 손실함수라고함(다차함수임). 이게 0에 가깝게, 즉 가장 작은 값이어야함. 그 작은 값은 함수를 미분했을 때 값이 0인 지점.  물론 다항다차함수라서 x1,2,3,4에 대해 편미분해야됨. 

미분 : 단항다차함수에 적용(x가 하나고 차수가 많음, 제곱이상)

편미분 : 다항다차함수에 적용가능(x여러개 제곱이상)

이렇게 오차를 줄여서(로컨 미니멈 함정 안빠지는 걸 고려해서) 가는걸 옵티마이저라고함.

그럼 왜 계단으로가냐? 적절한 시점에서 끊어줘야 가중치가 발산해거나 소실되서 학습이 안되기 때문. 계단안쓰면 가중치가 너무 커지거나 작아져서 학습이 안될수 있어서. 

![image.png](https://prod-files-secure.s3.us-west-2.amazonaws.com/914cd412-d6e9-4c46-8afb-88e9c6594671/537f1735-93c7-4e38-973b-de8ddd4d32aa/image.png)

[활성화 함수](https://hwk0702.github.io/ml/dl/deep%20learning/2020/07/09/activation_function/)

퍼셉트론에 대해 다시 공부해보자.

[07-01 퍼셉트론(Perceptron)](https://wikidocs.net/24958)

cpu는 실리콘으로 만듬. 그걸로 잉곳을 만듬. 그 잉곳만든거는 순수한 석영으로 만든 틀에다가 만들음. 순수한 실리콘이랑 석영은 미국에서…나옴. 미국사기. 모래를 녹여 잉곳을 만들고 잉곳을 얇게 잘라네 웨이퍼를 만듬(웨이퍼 1개가 10억). 이 잉곳을 잘라만든 웨이퍼가 하이닉스랑 삼성에서함. 웨이퍼는 파운드리(설계는 못하고 공정만함)라는 생산기지로 가거나 팹리스(공정하는 회사가 없음. 설계만 함)로 감. 웨이퍼 깎아내는 곳은 일본이랑 네덜란드(asml)에 있음. 공장으로 간 웨이퍼는 n나노 공정으로 각각 투입됨. 이걸 와플처럼 잘라냄(에칭). n나노에서 n나노는 전자길의 두께임. 작게만들면 값이 비싸지지만 집적도가 좋아져 성능이 증가하고 전력 소모가 감소함.  cpu에 연산기 회로 패턴을 노광기로 만들어냄(asml에서). cpu에 연산기 만들었으면 메모리컨트롤러, 캐시메모리,  내장 그래픽카드 도 넣어둠. cpu는 soc가 아님. 한 다이(웨이퍼 하나 잘라낸거)에 올려야 soc임. cpu는 반도체에 칩을 직접 올려서 속도가 빠름. cpu는 soc라고 안하고 chiplet이라고 부름. 내장그래픽이랑 cpu는 붙어있음(속도때문에). 그렇다고 다이 크게 만들면 수율이 안나옴(먼지같은거 때문에 망가질 위험이 커서). 신공정은 대체로 3%수율임. 공정개선으로 수율을 올림(공정미세화). 1다이에 4코어를 넣는다면 쿼드코어라고 판매할수있음. 그런데 먼지같은게 코어위에 올라가 코어가 죽으면 트리플, 더블, 싱글코어로 팔수있음.

cpu 회로 다 새기고 나면 패키징(포장작업)을 함. cpu에 접점(핀)을 만들어냄. 접점이 많으면 속도가 빨라짐. 대체로 cpu에 접점이 있기 보다는 메인보드에 접점을 만들음(LGA? 라고 한다네. CPU에 만들면 PGA라고 한다네? 생산성을 위해. 메인보드가 더 싸서 망가져도 교체가 가능하기 때문에). cpu위에 써멀 바르고 쿨러를 장착함. 쿨러까지 팍 때버리면 그대로 cpu까지 딸려뽑혀남(무뽑). 드라이기로 써멀 녹여서 살살 때낸다네… 패키징된 cpu는 잘 불량이 많이 난데. 이전에 만든 CISC,RISC 기반 회로를 새로 만들어냄(N나노 차이가 있어서 그래). 그런데 이게 옛날 명령어셋들 요즘에 안써서 검사를 대충한데. 그래서 고장 잘난데. 요즘 인텔14세대는 설계자체가 잘못되서 고장 많음. 특히 한국 개발자는 프로그램을 구형을 업데이트 안해서 잘 고장난데. 2나노 공정하지만 그 이하 10,14나노 공정도 계속 생산함. 안정성이 중요한 물건(자동차)에 쓰임. 삼성이 8나노로 돈번다고 한다네. 몇주차에 생상된 웨이퍼로 만든 램을 모아서 파는 회사(커세어)도 있데. 램은 대기업들 밖에 생산을 못해. 삼성하고 하이닉스가 메모리칩의 제어기(컨트롤러) 만드는데 최고임. 요즘은 그냥 사서 순정대로 쓰는게 좋다.

노광장비가 전력도 많이 쓰는데 열도 많이 나서 그걸로 지역난방을 돌린데.  

내일부터 옵티마이저, 경사하강법을 배울 것.

퀴즈

1.  인공지능을  AGI, NARROW로 분류하는데, 그 분류기준은?
2.  LLM(large laegue model)에서 왜 모델이란 단어를 쓰는가?
3.  도메인은 뭐고 예시는?
4.  LLM(large laegue model)에서 왜 large란 단어가 붙는가?
5.  머신러닝과 딥러닝을 분류하는 기준은? 그리고 학습법에 따른 3가지 분류는?
6.  뉴런의 동작원리는?
7.  입력값x 입계값 가중치w 활성홤수hx 역치?a 중 직접 제어할 수 있는 값은?
8.  왜 활성화 함수h(x)는 비선형 함수를 쓸까?
9.  왜 활성화 함수에 계단함수를 쓰는가?
10. 왜 가중치를 구하기 위해 편미분을 사용하는가?
- 정답
    1.  TASK
    2.   함수(funtion)이기 때문임. 입력→출력 이 나옴. 프레스기에 쓰는 틀도 모델이라고 볼수있음. 즉, 구조 설계임.
    3.  어느 한 분야를 말함. CONVERSION(대화), TRANSRATE(번역), TTS가 도메인. 국어,수학,과학,의학 등등 이런 분류를 도메인이라고함
    4.  옛날에는 키울수록 서능 좋아지다 성장 안하는데 이젠(LARGE)는 키울수록 성능이 계속해서 좋아지기 때문이다
    5.  모델따라서. 지도학습,비지도학습, 강화학습
    6.  가지돌기에서 전기신호 입력받아 역치를 넘기면 축삭돌기로 전기신호 전달
    7.  가중치w
    8.  활설화 함수가 일차함수이면, 일차함수를 아무리 곱해서 똑같은 일차함수가 나옴. 그러니 이차 이상인 함수를 곱해야됨.
    9.  계단함수를 써야 가중치가 극단적으로 커지거나 0에 가까워져서 학습이 안되는 경우를 방지함
    10. 체인룰을 이용해서 가중치를 쉽게 구할 수 있기 때문임.