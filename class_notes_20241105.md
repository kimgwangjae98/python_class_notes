오늘은 머신러닝을 주로 나가고 다른 내용도 조금씩 추가해서 강의를 진행할 것임. 딥러닝 머신러닝을 병행함. 기초는 착실히 쌓아서 빠르게 나감. 파이토치 쓸거임. 파이토치가 환경을 덜 타고 오픈소스를 씀. 그래서 씀. 밑바닥 딥러닝 책을 완벽히 이해할 수 있도록 계속 읽어보기. 번역본이라 어색한 문장도 있음. 퍼셉트론-신경망-신경망학습-오차역전파법-학습기술-CNN 을 배움. 1장,8장 안배움. 단단한 머신러닝 책으로 머신러닝을 배울 것임. 머신러닝 책에서 모든 걸 배울것임. 양이 많지만 좋은 내용들 뿐이라 그럼. 

우리는 지금 인공지능 붐을 경험하고있음. 그러니 정확한 지식을 가지고 있으면 좋음. 인공지능은 인간이 아닌 디지털로 학습을 하는 행위를 말함. 인공지능은 두 가지 분류로 나뉨. 그중 하나인 머신러닝이 있음. 말그대로 기계를 통한 학습임. 그리고 딥러닝도 있음. 이건 기계를 신경망을 이용해 학습함. 여기서 학습이란 뭘까? 학습이란 규칙을 알아가는 과정(규칙을 습득하는 과정)을 말함. 이 학습에는 룰베이스, 인간이 정해놓은 규칙에 따라 움직이는 것이다. 이 규칙을 습득하는 방법이 신경망을 쓰는지 안쓰는지에 따라 달라지지. 머신러닝과 딥러닝 외에도 강화학습이 있다. 이것은 가상의 world에서 보상을 주어 학습한다. 인공지능 안에 머신러닝이 있고 머신러닝 안에 딥러닝이 있다. 강화학습은 인공지능, 머신러닝, 딥러닝을 모두 포함한다. 그러니까 가오하학습은 인공지능의 일부이고, 머신러닝과 딥러닝은 강화학습의 일부이다. 학습이란 규칙을 알아가는 과정이다 신경망 유무에 따라 구분한다는 질문에 답을 할 수 있게 됨.

선형 모델에서 '모델'은 무엇인가? 학습의 주체이다. 선형 모델은 규칙을 선형적으로 판단한다. 이게 무슨말이면, yes or no 로 판단한다는 소리다. 선을 경계로 yes or no로 판단하지. 편가르기로 알면 좋다. 

의사결정 트리는 decision tree 라고 한다. 이것은 내가 어떠한 선택에 대한 분기가 갈린다. 둘 중에서 어느 것을 선택할 지는 학습을 통해 결정한다. 

퍼셉트론은 신경망이 아니다. 다층 퍼셉트론이 신경망이다. 단층 퍼셉트론은 신경망이 이니다. 단층은 선형모델이고, 다층은 비선형 모델이다. 일반적으로 퍼셉트론은 단층 퍼셉트론을 의미함.

서포트 벡터 머신은 선형 모델이다. 선으로 갈랐을 때 선과 점의 거리를 떨어지도록 만든다. 

베이지안 분류기는 확률과 통계에서 배움. 이것만 배우면 된다. 베이지안이란 전에 있던 결과가 현재에 얼마나 영향을 끼치느냐를 알려줌. 과거를 통해 미랠르 예측하는 모델이다.

앙상블 학습은 ensumble이라고 모델 여러개를 묶어서 쓴다고 보면 된다. 성능이 좋아지냐는 모름. 마냥 좋은건 아니다.

위 모델들, 학습들은 정답지(라벨)이 존재했음. 아래 모델, 학습들은 정답지가 없음. 

클러스터링은 걔네들 간의 특징들로 구분하지.뭔지는 몰라도 묶을 수 있음. 비지도 학습이다. 특징을 뽑아내서 비슷한 것들끼리 뭉치게 만든다.

차원 축소는 PCA라고 부름. 클러스터링을 하기 위해 탄생했다. 특징은 벡터로 표현될 수 있다. 100차원이면 너무 많으니까 차원을 축소할 필요가 있음(단순화). 리소스(성능)이 낮아져도 학습이 가능해짐.

특성 선택은 명확한 특성을 가지도록 해줌.

희소 학습은 데이터가 적을 때 어떻게 학습하는지 방법을 줌. 보통 비지도 학습이다.

계산 학습 이론은 차원 축소를 할 때 어떻게 학습시키느냐에 대한 이론을 말함.

준지도 학습은 semi 지도학습이다. 지도+비지도 학습이 섞여있는, 라벨이 일부만 있는 학습이다. 

챕터 들어갈 때 지도학습인지 아닌지 확인하고 공부하기.

확률 그래피컬 모델은 그래프 이론임. 그래프를 학습시키는 모델이다. 점과 점 사이가 이어진 모습을 보고 그래프라고 함. 쉽게 말하자면 최적의 길찾기 모델

강화 학습도 있다.

역사
40년대에 cpu만 존재하는 컴퓨터가 존재함. 룰베이스 모델로 기계가 움직였지. 그래서 퍼셉트론 모델이 나왔다(선형모델 출현). 그담에 암흑기가 옴. 병렬연산이 너무 커서 cpu가 감당을 하지 못했다. 오차역전파법이 없어 학습도 못했지. 그런데 이거쓰니까 가중치 소멸도 와서 안됬다. 그거 해결하는 cnn등등 쓰다가 다시 암흑기. 하드웨어 성능이 좋지 않았기 때문임. 그게 기술발전이 해결해줬다. 

인공지능은 인간의 경험적 지식이 적용되는 방면, 반복 작업에 효과적이다. 그 외에는 그냥 기존에 있던거 쓰면 되.

참고로 머신러닝책은 공대 대학생, 대학원생을 위한 책임. 흐름을 이해할려하기.

변수집합은 스칼라 모인거. 단위 행렬은 111 대각선 그거. 샘플 공간 혹은 상태 공간은 벡터가 형성하는 공간. span임. 데이터 샘플은 데이터. 가설 공간(span)이랑 가설 집합은 예측임.
노름은 중요함. L1,L2 norm 표시함.

기록들의 집합을 데이터 세트 라고 한다. 사진에 여러장 있는 거지. 사례 혹은 샘플은 사진 한장을 의미하고, 사진이 붉다는 형식의 속성특징을 속성 혹은 특성(feature) 라고 한다. 피쳐가 크다 란 뜻한 벡터가 크다 란 뜻. 이는 차원이 크다 란 뜻, 즉 특징을 이루는 벡터가 많다는 뜻. 특성 벡터는 특성을 벡터로 표현한 것. 
가설은 모델이라고 봐도 됨. 라벨은 그라운드 쓰루스(진실. gt)이라고 말하기도함. 예측(프리딕션)은 결과를 예측한다는 뜻이고 추론(인퍼런스)이라고도 함. 분류는 분류에 대한 테스트를 말함. 모델의 목적이 분류다. 불연속적이지. 회귀는 어떠한 값에 수렴함을 말함. 연속적임. 클러스터링은 라벨(이름)이 없음. 라벨있으면 분류임. 지도 학습은 라벨이 있고 비지도 학습은 라벨이 없고. *일반화가 중요함. 어떤 상황에서도 쓰일 수 있음을 의미함.* 귀납적 추론은, 귀납은 내가 가진 데이터를 통해 특징을 뽑아내서 학습하는 것. 연역은 어떠한 반례를 찾아서 맞는지 아닌지 보는 것. 그러니까 수학적 공리 시스템에서 공리와 추론 규칙에 기반을 두고 이에 일치하는 수학적 정리를 유도하는 것. 증명할 때만 나오니 그냥 귀납, 연역의 의미만 알아두기. 가설(모델)을 세울 때 현재 데이터에 잘 맞아야된다. 이를 피팅이라고 부른다. 그래서 학습 과정을 피팅 혹은 러닝이라고 부른다. 가설을 바꾼다는 것은 가중치를 바꾼다는 것이지. 이렇게 계속 가설을 바꿔서 잘 맞는걸 찾는 과정을 러닝 혹은 피팅 이라고 하는 거지. 귀납적 편향(bias). 편향은 한쪽으로 치우쳐짐을 말함. 이는 그 데이터에 대해 특징이 치우쳐짐을 의미함. 편향이 잘못된것은 아닌데 틀릴때도 있지. 폐렴을 확인하랬더니 폐의 크기만 보고 판단해버린거지. 이게뭐여... 데이터 마이닝, 마이닝은 채굴을 의미함. 데이터에서 정보를 분석하는 것을 데이터 마이닝이다. 즉 데이터에서 특징을 파악하란 의미다. 이 사진의 70쪽에 특징이 몰렸다 이렇게.

전이 학습과 파인 튜닝의 차이는? 
전이 학습 : 딥러닝에 쓰는 단어. 딥러닝은 특징 추출기와 신경망으로 이루어짐. 전이 학습은 기존 특징추출기에다가 새로운 신경망을 학습한다.
파인 튜닝 : 특징 추출기와 신경망 둘다 가져와서 미세조정을 한다.
백본 네트워크에서 백본은 척추, 즉 특징추출기를 말함. 백본 네트워크는 CNN에서 사용함. 그냥 백본 네트워크=특징 추출기 라고 이해하기.
gan은 갠 혹은 간 이라고 읽음. 

모델(가설) 평가는 손실함수(오차를 구함)로 함. 오차의 정도를 구함. 선택은 오차가 적은걸 선택하면 된다. 과적합은 일반화 성능이 낮다, 현재 데이터에 만 잘 맞는다는 뜻. 언더피팅은 피팅(학습)이 낮다. 즉 가설 보다 아래다. 성능이 낮다는 뜻으로. 특징을 충분히 학습하지 못했다는 의미이다. 즉 학습이 덜됐다! 

에포크랑 이터레이션 차이
에포크는 100개의 데이터(배치)를 새로운 가설을 갱신하는데 썻을 때를 말함. 전체 데이터에 대한 순전파 역전파가 끝난 상태. 이러면 1에폭임. 
이터레이션은 1개 데이터를 새로운 가설 갱신하는데 썼을 때 횟수. 이러면 1이터=1에폭. 만약 1만개 데이터를 100개 배치로 나눠 100번 학습 시키면 100이터=1에폭. 이터레이션이라고 표현을 안 할 수도 있음. 
배치가 작으면 그 데이터를 충분히 대표할 수 없을 수 있음. 그래서 배치 사이즈가 중요함. 그래서 배치 사이즈 큰거 쓸려고 컴퓨터 성능 좋은거 쓸려고하지. 웬만하면 배치 사이즈가 클수록 좋긴함. 

램이 많으면 모델의 크기가 커짐. 또 학습끝난 모델의 가설(예측)할 때 좋다는거지. 배치 사이즈가 커지는 것도 있지만. 

홀드아웃은 겹치지 않게 두 집합으로 나누는 방법. 테스트와 트레인 테이터가 겹치면 안됨. 안그럼 뚝빼기 깨면 됨. 수능에 기출문제 나온거라 하면안됨.

벨리데이션은 검증이란 뜻임. 왜 하냐, 테스트 데이터는 한번 쓰면 끝이라서 트레인 데이터 셋과 벨리데이션 데이터셋과 데스트 데이터넷을 나눔. 오버피팅을 판단하기 위해 사용함. 벨리제이션 데이터를 입력했을 때 성능이 떨어지는 구간을 오버피팅 구간이라고 판단할 수 있음. 그때 학습을 멈추는 방식을 얼리스탑이라고 함. 얼리스탑 기준은 자기가 정해야됨. 아니면 오차만 보고 멈출 수 있다.

크로스 벨리데이션은 내가 테스트셋을 뽑았는데 전체를 대표할 수 있나? 그러니까 테스트셋과 트레인셋을 나눴는데 테스트셋은 전체 데이터셋을 대표할까? 아닐수도 있어서 k-fold cross validation이 나옴. k번 나눈다는 뜻임. 10개로 데이터 나눠서 9개는 트레인, 1개는 테스트 셋으로 나누고, 1~10번째 테스트 세트는 매번 새로운 모델에 적용해 결과를 내고 평균을 내서 검증하는거지. 이건 그림 참조. LOOCV라고 부르기도함.

부스팅이 뭐지. 데이터가 많이 부족할 때 씀. 10개의 데이터를 9개 트레인, 1개 테스트로 나누고 9개의 트레인에서 1개를 벨리데이션으로 지정한 뒤 k-fold 교차검증 시전. 이러면 데이터를 온전히 쓸 수 있어서 좋다. 데이터가 적어도!

하이퍼 파라미터는 내가 정할 수 있는 숫자를 말함. 배치사이즈, 학습률 같은거. 이런거는 엄청 많음. 가중치도 하이퍼 파라미터라고 함. 

성능 측정할 때 쓰는 기법들이 있음. MSE, MAE 이거. 회귀(리그레션)에서는 MSE, MAE를 쓰고 바이너리 분류(클레시피게이터)에서는 realll pression FI score accurray 등을 씀. 다중 분류에서는 accurracy를 씀. 그래서 모델 만들때도 저 성능 측정 기법을 잘 선택해야됨. ROC, AUC는 다른 말. 이것도 성능 측정할 때 씀. ROC는 나도 검색해서 본 적이 있다. 아무튼 성능 측정 기표는 많다!

웬만하면 흐름 가는대로만 알기.

비교검증은 데이터셋의 무결성과 일반화 성능을 검증하는데 쓰임. 데이터 자체는 굉장히 확률적인 분포로 이해해야됨. 내가 잘된 케이스만 넣어버리면 성능100% 달성은 빠름. 이런 폐쇄된 데이터셋을 쓰면 사기나 다름없지. 그래서 통계가설 검정을 통해 확률적으로 성능을 냈다고 하면 논문쓸때나 실제 제품 개발에 일반화 성능을 볼 수 있음. 통계가설 검정은 진짜 중요함.  만테스트,프리맨, u분포,t분포 등으로 검증을 할 수 있음. 학습한특징에 대해서 정규분포를 띈다. 그러면 정규분포를 띄는 데이터셋은 어떻게 뽑아? 예시를 보자. 사이킬런으로 머신러닝 시킴. 코드3줄이면 끝난다네. 다만 cpu만 지원한다는 치명적인 단점이 존재함. 머신러닝은 선형이라 간단하게 결과가 나와 역으로 추적이 가능하지만, 딥러닝은 비선형이라 추적이 매우 어려워 불가능하다. 검증을 통해 데이터 처리를 하여 성능을 높일 수 있지. 사이킬런으로 좋은 데이터셋을 만들 수 있다. 유의미한 특징을 하나 잡아서 딥러닝을 학습시키는데 사용해야함. 데이터 전처리하면 선응이 180도 바뀜. 사이킬런으로 전처리 - 파이토치로 학습. 이 중요함. 그만큼 데이터셋을 직접 보고 직접 걸러네면 엄청난 성능 향상을 할 수 있음. 모델은 빨리 만드는데 데이터가 시간 오래 걸림. 

아웃라이너도 중요함. outliner 라고 하는데 예외값, 즉 튀는값을 말함. 평균과 분산과 편향을 알거임. 분산은 평균에서 벗어난 정도를 뜻함. 평균에서 너무 벗어난 것은 전문가가 직접 확인을 해보고 제거를 해야됨. 무작정 데이터셋을 제거하면 안됨. IQR 제거방법이 있는데 1~4분위 빼고 날리는것. 뭐에 대해서 아웃라이너인지에 대한 기준은 본인이 직접 설정해야됨. 픽셀값으로 정한다거나 갑자기 4k사진이 들어왔다던가. 사진도 1억화소를 1천만화소로 변환한다면 용량은 용량대로 줄이고 기존 특징들을 압축할 수 있기도함. 이거는 기준을 알아서 잘 정해야됨. 가장 데표적인 데이터셋이 붓꽃 데이터셋.
통계보기-아웃라이너제거-이런식으로

선형 모델 기본 형식은 백터 내적할때랑 같음. 가중지랑 입력 벡터 집합의 내적이 fx로 나옴. wT는 전치해줘야 벡터내적이 가능해서 T로 전치가 붙음. 그런데 왜 굳이 전치를 할까? x는 입력값임. x는 여러개로 분해되거나 속성이 여러개라 벡터로 표시함. 특징이 4개면 벡터도 4개가 나옴. 그러면 가중치도 특징 갯수에 맞게 4개가 생성됨. 전치는 내적 계산을 하기 위해 씀. 1 * 4, 1 * 4 나오는데 w를 전치해야 내적계산이 가능하지. 로 나우는데  선형 모델의 기본 형식은 일차식으로 표현되는 차수가 1개일 때임. 선형 모델은 선형 함수 혹은 선형 방정식이라고 부름. 선형 회귀는 어떠한 값으로 가까워짐. w와 b를 조정해서 특정한 값에 가까워지도록함. 선형 회귀는 결국 어떤값아 다가감. 얼마만큼 답과 차이가 있는지 MSE MAE를 써서 구함.  유클리드 거리는 그냥 원으로 표현됨. 오차가 최소가 되도록 하면 됨. 흐름만 이해하기. 로그 선형 회귀는 그냥 결과값이 로그 씌운거 그게 끝. 미분쉽게할려고 로그 씌워서 함. 근사하다고 보고 하는거임 그저. 이걸로 일반해를 구할 수 있다는 정도만 알기. 로지스틱 회귀는 왜 배우냐? 신경망의 계단함수와 같아서 배움. 선형함수에 어떤걸 곱하면 그 함수 자신이 나옴. 시그모이드 함수는 외워두라고한거 그거. 로지스틱 회귀를 통해 데이터가 어떠한 클래스에 얼마나 속하는지도 알수있음.로지스틱은 비선형 함수로 회귀를 한다는거임.  LDA는 선형 판별 분석이란 소리. 선형 학습법 중 하나로 머신러닝에서는 LDA로 가설(모델)을 갱신함. 특징들을 선에 투영시켜 뭉치게 했을 때 같은 특징은 서로 가깝게 뭉치고, 다른 특징 뭉치랑 멀리 떨어져야됨. 국수 모아둔거 떨어뜨려놓는거지. 가중치 w조정해서 오차를 최대한 작게 만들고 서로 뭉치게 만드는거지. 그냥 간단하게 머신러닝 가중치w 갱신법이라고 만 알기.